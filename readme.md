# LLM Deployment Examples

A comprehensive collection of production-ready examples for deploying Large Language Models (LLMs) and machine learning models using various technologies and architectures.

## What is this?

This repository provides practical, working examples of different approaches to deploying LLMs and ML models in production environments. Each example demonstrates real-world deployment patterns, from simple HTTP servers to complex distributed systems with load balancing, monitoring, and scaling capabilities.

## What's included?

- **HTTP/GRPC Services**: Basic and advanced API implementations
- **Streaming Solutions**: Server-Sent Events (SSE) and bidirectional streaming
- **Model Serving**: Triton Inference Server, vLLM, and OpenVINO deployments
- **Containerization**: Docker and Kubernetes configurations
- **Monitoring & Observability**: Prometheus, Grafana, and performance testing
- **Load Balancing**: Nginx configurations and scaling strategies
- **Production Patterns**: Health checks, metrics, and deployment best practices

## Who is this for?

- **ML Engineers** looking to deploy models in production
- **DevOps Engineers** working with ML workloads
- **Software Engineers** building AI-powered applications
- **Anyone** interested in learning modern ML deployment patterns

## Getting Started

Each directory contains a self-contained example with its own README, requirements, and setup instructions. Choose the example that matches your use case and follow the instructions in that directory.

## Technologies Covered

- **Model Serving**: Triton, vLLM, OpenVINO
- **Communication**: HTTP, gRPC, WebSockets, Server-Sent Events
- **Containerization**: Docker, Docker Compose, Kubernetes
- **Monitoring**: Prometheus, Grafana
- **Load Balancing**: Nginx
- **Languages**: Python, JavaScript, Protocol Buffers

## Contributing

Feel free to submit issues, feature requests, or pull requests to improve these examples.

## SOON (not completed yet)
- `9_openvino`
- `11_triton_vllm_backend`