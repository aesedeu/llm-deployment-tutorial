{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f37fdc2",
   "metadata": {},
   "source": [
    "# dummy classic ml model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d28ddd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "\n",
    "X, y = make_regression(1000, 3)\n",
    "\n",
    "model = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ba6ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([506.48185252])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df8098b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# классическое сохранение в joblib\n",
    "joblib.dump(model, \"model.joblib\")\n",
    "\n",
    "# загрузка из joblib\n",
    "# joblib.load(\"model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eb702e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: ['input']\n",
      "Outputs: ['variable']\n"
     ]
    }
   ],
   "source": [
    "# from skl2onnx import convert_sklearn\n",
    "import onnx\n",
    "from skl2onnx import to_onnx\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "initial_type = [(\"input\", FloatTensorType([None, X.shape[1]]))]\n",
    "# onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
    "onnx_model = to_onnx(model, initial_types=initial_type)\n",
    "\n",
    "with open(\"model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"Inputs:\", [inp.name for inp in onnx_model.graph.input])\n",
    "print(\"Outputs:\", [out.name for out in onnx_model.graph.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.post(\n",
    "    url=\"http://localhost:8080/triton_classic_ml\",\n",
    "    json={\"a\": 1, \"b\": 2, \"c\": 3},\n",
    "    params={\"model_name\": \"classic_model\", \"model_version\": 1}\n",
    ")\n",
    "\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "283991ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    r = requests.post(url=\"http://localhost:8080/classic\", json={\"a\":2, \"b\": 2000, \"c\": 10})\n",
    "    r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a05e5",
   "metadata": {},
   "source": [
    "# dummy LLM (BERT) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ad20e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aweeu/Desktop/docker_llm_webinar/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель содержит параметры следующих типов: {torch.float32}\n",
      ": <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "embeddings: <class 'transformers.models.bert.modeling_bert.BertEmbeddings'>\n",
      "embeddings.word_embeddings: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.position_embeddings: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.token_type_embeddings: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "embeddings.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "embeddings.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder: <class 'transformers.models.bert.modeling_bert.BertEncoder'>\n",
      "encoder.layer: <class 'torch.nn.modules.container.ModuleList'>\n",
      "encoder.layer.0: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.0.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.0.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.0.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.0.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.0.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.0.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.0.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.0.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.0.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.0.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.0.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.1.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.1.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.1.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.1.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.1.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.1.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.1.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.1.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.1.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.1.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.1.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.2.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.2.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.2.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.2.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.2.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.2.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.2.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.2.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.2.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.2.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.2.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.3.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.3.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.3.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.3.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.3.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.3.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.3.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.3.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.3.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.3.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.3.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.4.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.4.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.4.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.4.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.4.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.4.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.4.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.4.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.4.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.4.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.4.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.5.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.5.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.5.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.5.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.5.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.5.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.5.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.5.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.5.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.5.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.5.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.6.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.6.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.6.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.6.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.6.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.6.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.6.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.6.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.6.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.6.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.6.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.7.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.7.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.7.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.7.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.7.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.7.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.7.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.7.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.7.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.7.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.7.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.8.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.8.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.8.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.8.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.8.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.8.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.8.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.8.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.8.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.8.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.8.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.9.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.9.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.9.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.9.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.9.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.9.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.9.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.9.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.9.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.9.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.9.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.10.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.10.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.10.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.10.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.10.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.10.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.10.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.10.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.10.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.10.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.10.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11: <class 'transformers.models.bert.modeling_bert.BertLayer'>\n",
      "encoder.layer.11.attention: <class 'transformers.models.bert.modeling_bert.BertAttention'>\n",
      "encoder.layer.11.attention.self: <class 'transformers.models.bert.modeling_bert.BertSdpaSelfAttention'>\n",
      "encoder.layer.11.attention.self.query: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.key: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.value: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.self.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.attention.output: <class 'transformers.models.bert.modeling_bert.BertSelfOutput'>\n",
      "encoder.layer.11.attention.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.attention.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.11.attention.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "encoder.layer.11.intermediate: <class 'transformers.models.bert.modeling_bert.BertIntermediate'>\n",
      "encoder.layer.11.intermediate.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.intermediate.intermediate_act_fn: <class 'transformers.activations.GELUActivation'>\n",
      "encoder.layer.11.output: <class 'transformers.models.bert.modeling_bert.BertOutput'>\n",
      "encoder.layer.11.output.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "encoder.layer.11.output.LayerNorm: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "encoder.layer.11.output.dropout: <class 'torch.nn.modules.dropout.Dropout'>\n",
      "pooler: <class 'transformers.models.bert.modeling_bert.BertPooler'>\n",
      "pooler.dense: <class 'torch.nn.modules.linear.Linear'>\n",
      "pooler.activation: <class 'torch.nn.modules.activation.Tanh'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained(\"ai-forever/ruBert-base\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ai-forever/ruBert-base\", do_lower_case=True)\n",
    "\n",
    "dtypes = set(param.dtype for param in model.parameters())\n",
    "print(f\"Модель содержит параметры следующих типов: {dtypes}\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(f\"{name}: {type(module)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980b724",
   "metadata": {},
   "source": [
    "## сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aweeu/Desktop/docker_llm_webinar/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encodings = tokenizer.encode_plus(\n",
    "    \" \".join([\"word\"] * 10),\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "output = model(**encodings)\n",
    "# output.pooler_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ad46a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aweeu/Desktop/docker_llm_webinar/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успешно экспортирована в файл: model.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "encodings = tokenizer.encode_plus(\n",
    "    \" \".join([\"word\"] * 10),\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "dummy_input = (encodings[\"input_ids\"], encodings[\"attention_mask\"], encodings[\"token_type_ids\"]) # Пример входного тензора\n",
    "onnx_file_path = \"model.onnx\"  # Путь для сохранения ONNX файла\n",
    "\n",
    "# Экспорт модели\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model,  # Модель PyTorch\n",
    "        dummy_input,  # Пример входных данных\n",
    "        onnx_file_path,  # Путь для сохранения ONNX файла\n",
    "        export_params=True,  # Экспортировать обученные параметры\n",
    "        opset_version=14,  # Версия ONNX операторов\n",
    "        do_constant_folding=True,  # Оптимизация констант\n",
    "        input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "        output_names=[\"pooler_output\"],\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "            \"attention_mask\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "            \"token_type_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "            \"pooler_output\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )\n",
    "\n",
    "print(f\"Модель успешно экспортирована в файл: {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe477cb",
   "metadata": {},
   "source": [
    "## квантизация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33ed7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "\n",
    "torch.backends.quantized.engine = \"qnnpack\"  # Intel/AMD: \"fbgemm\"\n",
    "\n",
    "quantized_model = quantize_dynamic(\n",
    "    model=model,\n",
    "    qconfig_spec={torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc3de89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 size: 713.23 MB\n",
      "INT8 size: 92.70 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"FP32 size: {sum(p.numel() for p in model.parameters()) * 4 / 1e6:.2f} MB\")\n",
    "print(f\"INT8 size: {sum(p.numel() for p in quantized_model.parameters()) * 1 / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "model_name = \"ai-forever/ruBert-base\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Пример входных данных\n",
    "inputs = tokenizer(\"Тестовое предложение\", return_tensors=\"pt\")\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"last_hidden_state\", \"pooler_output\"]\n",
    "\n",
    "# Экспорт в ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "    \"rubert_base.onnx\",\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset_version=13,\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "        \"last_hidden_state\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "        \"pooler_output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4803d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "INT4",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnxruntime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantize_dynamic, QuantType\n\u001b[32m      3\u001b[39m quantize_dynamic(\n\u001b[32m      4\u001b[39m     model_input=\u001b[33m\"\u001b[39m\u001b[33mmodel.onnx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     model_output=\u001b[33m\"\u001b[39m\u001b[33mrubert_base_quant.onnx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     weight_type=QuantType.QInt8,  \u001b[38;5;66;03m# Можно использовать QuantType.QUInt8\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/docker_llm_webinar/.venv/lib/python3.11/site-packages/onnxruntime/quantization/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcalibrate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      2\u001b[39m     CalibraterBase,\n\u001b[32m      3\u001b[39m     CalibrationDataReader,\n\u001b[32m      4\u001b[39m     CalibrationMethod,\n\u001b[32m      5\u001b[39m     MinMaxCalibrater,\n\u001b[32m      6\u001b[39m     create_calibrator,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqdq_quantizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QDQQuantizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquant_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuantFormat, QuantType, write_calibration_table  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/docker_llm_webinar/.venv/lib/python3.11/site-packages/onnxruntime/quantization/calibrate.py:22\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelProto, TensorProto, helper, numpy_helper\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnxruntime\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquant_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_plot, load_model_with_shape_infer, smooth_distribution\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrel_entr\u001b[39m(pk: np.ndarray, qk: np.ndarray) -> np.ndarray:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    See https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.rel_entr.html#scipy.special.rel_entr.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    Python implementation.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/docker_llm_webinar/.venv/lib/python3.11/site-packages/onnxruntime/quantization/quant_utils.py:153\u001b[39m\n\u001b[32m    143\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    144\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m()  \u001b[38;5;66;03m# noqa: B904\u001b[39;00m\n\u001b[32m    147\u001b[39m ONNX_TYPE_TO_NP_TYPE = {\n\u001b[32m    148\u001b[39m     onnx_proto.TensorProto.INT8: numpy.dtype(\u001b[33m\"\u001b[39m\u001b[33mint8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    149\u001b[39m     onnx_proto.TensorProto.UINT8: numpy.dtype(\u001b[33m\"\u001b[39m\u001b[33muint8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    150\u001b[39m     onnx_proto.TensorProto.INT16: numpy.dtype(\u001b[33m\"\u001b[39m\u001b[33mint16\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    151\u001b[39m     onnx_proto.TensorProto.UINT16: numpy.dtype(\u001b[33m\"\u001b[39m\u001b[33muint16\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    152\u001b[39m     onnx_proto.TensorProto.FLOAT8E4M3FN: float8e4m3fn,\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43monnx_proto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensorProto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mINT4\u001b[49m: int4,  \u001b[38;5;66;03m# base_dtype is np.int8\u001b[39;00m\n\u001b[32m    154\u001b[39m     onnx_proto.TensorProto.UINT4: uint4,  \u001b[38;5;66;03m# base_dtype is np.uint8\u001b[39;00m\n\u001b[32m    155\u001b[39m }\n\u001b[32m    157\u001b[39m ONNX_INT_TYPE_RANGE = {\n\u001b[32m    158\u001b[39m     onnx_proto.TensorProto.UINT8: (numpy.array(\u001b[32m0\u001b[39m, dtype=numpy.uint8), numpy.array(\u001b[32m255\u001b[39m, dtype=numpy.uint8)),\n\u001b[32m    159\u001b[39m     onnx_proto.TensorProto.INT8: (numpy.array(-\u001b[32m128\u001b[39m, dtype=numpy.int8), numpy.array(\u001b[32m127\u001b[39m, dtype=numpy.int8)),\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m     onnx_proto.TensorProto.INT4: (numpy.array(-\u001b[32m8\u001b[39m, dtype=int4), numpy.array(\u001b[32m7\u001b[39m, dtype=int4)),\n\u001b[32m    164\u001b[39m }\n\u001b[32m    166\u001b[39m ONNX_INT_TYPE_SYMMETRIC_RANGE = {\n\u001b[32m    167\u001b[39m     onnx_proto.TensorProto.UINT8: (numpy.array(\u001b[32m0\u001b[39m, dtype=numpy.uint8), numpy.array(\u001b[32m254\u001b[39m, dtype=numpy.uint8)),\n\u001b[32m    168\u001b[39m     onnx_proto.TensorProto.INT8: (numpy.array(-\u001b[32m127\u001b[39m, dtype=numpy.int8), numpy.array(\u001b[32m127\u001b[39m, dtype=numpy.int8)),\n\u001b[32m    169\u001b[39m     onnx_proto.TensorProto.UINT16: (numpy.array(\u001b[32m0\u001b[39m, dtype=numpy.uint16), numpy.array(\u001b[32m65534\u001b[39m, dtype=numpy.uint16)),\n\u001b[32m    170\u001b[39m     onnx_proto.TensorProto.INT16: (numpy.array(-\u001b[32m32767\u001b[39m, dtype=numpy.int16), numpy.array(\u001b[32m32767\u001b[39m, dtype=numpy.int16)),\n\u001b[32m    171\u001b[39m }\n",
      "\u001b[31mAttributeError\u001b[39m: INT4"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=\"model.onnx\",\n",
    "    model_output=\"rubert_base_quant.onnx\",\n",
    "    weight_type=QuantType.QInt8,  # Можно использовать QuantType.QUInt8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588eaea",
   "metadata": {},
   "source": [
    "Конвертация модели в tensorrt\n",
    "\n",
    "https://docs.nvidia.com/deeplearning/tensorrt/latest/reference/command-line-programs.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1f21f",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker exec -it trtexec_container bash\n",
    "\n",
    "trtexec --onnx=model.onnx --help\n",
    "\n",
    "trtexec \\\n",
    "    --onnx=model.onnx \\\n",
    "    --saveEngine=model.plan \\\n",
    "    --minShapes=input:1x3 \\\n",
    "    --optShapes=input:8x3 \\\n",
    "    --maxShapes=input:16x3 \\\n",
    "    --fp16 \\\n",
    "    --useSpinWait\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b87a7e",
   "metadata": {},
   "source": [
    "## пример обращения по API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fdae19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding': [-0.4801458716392517,\n",
       "  0.13364841043949127,\n",
       "  -0.058410122990608215,\n",
       "  0.26510411500930786,\n",
       "  0.20883221924304962,\n",
       "  -0.06469129025936127,\n",
       "  1.700914978981018,\n",
       "  -0.27123305201530457,\n",
       "  0.4615176022052765,\n",
       "  0.8974533677101135,\n",
       "  0.6616724133491516,\n",
       "  0.7245630621910095,\n",
       "  0.03962193801999092,\n",
       "  0.670417845249176,\n",
       "  0.6577914357185364,\n",
       "  -0.16094788908958435,\n",
       "  0.6497836112976074,\n",
       "  0.7598335146903992,\n",
       "  -0.09708482027053833,\n",
       "  -0.33649399876594543,\n",
       "  -0.2876434624195099,\n",
       "  -0.29156625270843506,\n",
       "  1.0977646112442017,\n",
       "  -0.49361521005630493,\n",
       "  0.8974349498748779,\n",
       "  -0.5249939560890198,\n",
       "  -0.6229562163352966,\n",
       "  0.21012605726718903,\n",
       "  -0.47258394956588745,\n",
       "  0.2673443555831909,\n",
       "  0.12862682342529297,\n",
       "  0.06678874790668488,\n",
       "  -0.3363344669342041,\n",
       "  0.14100868999958038,\n",
       "  0.05123268440365791,\n",
       "  -1.5838018655776978,\n",
       "  0.06379370391368866,\n",
       "  0.15384118258953094,\n",
       "  -0.507167637348175,\n",
       "  0.1046915352344513,\n",
       "  -0.3378352224826813,\n",
       "  -0.656221866607666,\n",
       "  -0.5526315569877625,\n",
       "  -0.660680890083313,\n",
       "  -0.28252464532852173,\n",
       "  -0.01573564112186432,\n",
       "  0.6988096237182617,\n",
       "  -0.5787897109985352,\n",
       "  -0.46333348751068115,\n",
       "  0.5998961329460144,\n",
       "  0.731590211391449,\n",
       "  1.4046273231506348,\n",
       "  0.26682549715042114,\n",
       "  0.2542145550251007,\n",
       "  -0.5038730502128601,\n",
       "  0.11266402900218964,\n",
       "  -0.09244965016841888,\n",
       "  -0.3734170198440552,\n",
       "  -0.02012949436903,\n",
       "  -0.05142636597156525,\n",
       "  0.4270680546760559,\n",
       "  -0.21183671057224274,\n",
       "  -0.4672166109085083,\n",
       "  0.32921645045280457,\n",
       "  -0.8132289052009583,\n",
       "  0.6658265590667725,\n",
       "  -1.0187362432479858,\n",
       "  0.2621805965900421,\n",
       "  -0.17177267372608185,\n",
       "  1.4304312467575073,\n",
       "  -0.3940050005912781,\n",
       "  -0.029714442789554596,\n",
       "  0.02352830395102501,\n",
       "  0.28044915199279785,\n",
       "  -1.0670887231826782,\n",
       "  0.08535236865282059,\n",
       "  -1.025542974472046,\n",
       "  -0.036237891763448715,\n",
       "  0.49656274914741516,\n",
       "  -0.4963328242301941,\n",
       "  0.3212065100669861,\n",
       "  0.04444226995110512,\n",
       "  0.2843885123729706,\n",
       "  0.0062499940395355225,\n",
       "  -0.5096010565757751,\n",
       "  0.1226116269826889,\n",
       "  0.020954679697752,\n",
       "  0.6108595728874207,\n",
       "  0.5456339120864868,\n",
       "  0.680208683013916,\n",
       "  0.5810986161231995,\n",
       "  -0.1036323606967926,\n",
       "  0.19639858603477478,\n",
       "  0.5411901473999023,\n",
       "  0.7796894311904907,\n",
       "  -0.8455535769462585,\n",
       "  0.2539522349834442,\n",
       "  0.025131795555353165,\n",
       "  -0.44169262051582336,\n",
       "  -0.556596040725708,\n",
       "  -0.5602999925613403,\n",
       "  -0.512677013874054,\n",
       "  -0.1350267380475998,\n",
       "  0.7021392583847046,\n",
       "  -1.0893932580947876,\n",
       "  -0.75007164478302,\n",
       "  0.40781742334365845,\n",
       "  -0.1569296419620514,\n",
       "  -0.016239099204540253,\n",
       "  -0.2276894897222519,\n",
       "  0.6414549350738525,\n",
       "  0.00017251074314117432,\n",
       "  0.7286990880966187,\n",
       "  -0.2264716923236847,\n",
       "  -0.6161988973617554,\n",
       "  0.5892238616943359,\n",
       "  0.2562374472618103,\n",
       "  -0.050586022436618805,\n",
       "  1.1258037090301514,\n",
       "  -0.4169020652770996,\n",
       "  0.19645723700523376,\n",
       "  0.27607762813568115,\n",
       "  0.17000460624694824,\n",
       "  0.20520323514938354,\n",
       "  0.23724837601184845,\n",
       "  0.25655218958854675,\n",
       "  -0.2967824637889862,\n",
       "  -0.07343944907188416,\n",
       "  -0.23001420497894287,\n",
       "  -0.48299792408943176,\n",
       "  -0.5271966457366943,\n",
       "  0.38245880603790283,\n",
       "  0.5525771975517273,\n",
       "  0.059205312281847,\n",
       "  0.36137476563453674,\n",
       "  0.03256246820092201,\n",
       "  -0.12906943261623383,\n",
       "  -0.3905624449253082,\n",
       "  0.06154521554708481,\n",
       "  0.5420137643814087,\n",
       "  -0.7332568764686584,\n",
       "  -0.8073806762695312,\n",
       "  -0.01977691799402237,\n",
       "  -0.13599173724651337,\n",
       "  0.10585471987724304,\n",
       "  0.6537750363349915,\n",
       "  0.23128798604011536,\n",
       "  -0.04356418177485466,\n",
       "  -1.3538844585418701,\n",
       "  0.43098706007003784,\n",
       "  -0.5199757814407349,\n",
       "  -0.7823879718780518,\n",
       "  -0.145172581076622,\n",
       "  -1.2842878103256226,\n",
       "  0.6506023406982422,\n",
       "  0.4863457679748535,\n",
       "  0.39784348011016846,\n",
       "  0.34134408831596375,\n",
       "  0.8369345664978027,\n",
       "  -0.36373192071914673,\n",
       "  1.5554834604263306,\n",
       "  0.19271305203437805,\n",
       "  0.8039381504058838,\n",
       "  0.047916606068611145,\n",
       "  0.4341529607772827,\n",
       "  -0.9457468390464783,\n",
       "  1.0385371446609497,\n",
       "  0.2678319215774536,\n",
       "  0.7729957103729248,\n",
       "  0.37956923246383667,\n",
       "  -0.052492037415504456,\n",
       "  -0.37080758810043335,\n",
       "  -0.3491486608982086,\n",
       "  -0.2284465730190277,\n",
       "  -0.08324013650417328,\n",
       "  0.2436424195766449,\n",
       "  0.6326231956481934,\n",
       "  0.34204035997390747,\n",
       "  -0.2297930270433426,\n",
       "  0.07424397021532059,\n",
       "  -0.7682808041572571,\n",
       "  -0.030208580195903778,\n",
       "  0.28609344363212585,\n",
       "  -0.1240469366312027,\n",
       "  0.6610593199729919,\n",
       "  0.4225383996963501,\n",
       "  -0.3117077052593231,\n",
       "  0.23554731905460358,\n",
       "  -0.2513185441493988,\n",
       "  -0.07129961252212524,\n",
       "  0.6185327172279358,\n",
       "  -0.24048906564712524,\n",
       "  0.7038940787315369,\n",
       "  -0.18290981650352478,\n",
       "  -0.8482910394668579,\n",
       "  -0.235741525888443,\n",
       "  -0.5827462077140808,\n",
       "  -0.762426495552063,\n",
       "  -0.8450890183448792,\n",
       "  -0.42026016116142273,\n",
       "  -0.36426928639411926,\n",
       "  -0.5680732727050781,\n",
       "  0.6240354180335999,\n",
       "  -0.3491430878639221,\n",
       "  -0.4260944128036499,\n",
       "  -0.7555640339851379,\n",
       "  0.6036609411239624,\n",
       "  -0.25075680017471313,\n",
       "  -0.16110914945602417,\n",
       "  0.3843430280685425,\n",
       "  1.0883398056030273,\n",
       "  -0.08200251311063766,\n",
       "  0.16378140449523926,\n",
       "  -0.05045139417052269,\n",
       "  0.34986740350723267,\n",
       "  0.36412858963012695,\n",
       "  -0.814873993396759,\n",
       "  0.6768308281898499,\n",
       "  -0.9118483066558838,\n",
       "  0.22246505320072174,\n",
       "  -0.5078978538513184,\n",
       "  -0.07856906950473785,\n",
       "  0.6416600346565247,\n",
       "  -0.3306826055049896,\n",
       "  -0.9518921971321106,\n",
       "  0.37464645504951477,\n",
       "  -0.21885469555854797,\n",
       "  -0.31754764914512634,\n",
       "  0.21943461894989014,\n",
       "  -0.5363903045654297,\n",
       "  -0.7696376442909241,\n",
       "  -0.6224323511123657,\n",
       "  0.13575813174247742,\n",
       "  0.035207346081733704,\n",
       "  -0.1795603334903717,\n",
       "  0.15539132058620453,\n",
       "  -0.9392162561416626,\n",
       "  -0.4210051894187927,\n",
       "  -0.195771723985672,\n",
       "  -0.3855888247489929,\n",
       "  -1.7487475872039795,\n",
       "  0.14901591837406158,\n",
       "  0.42789578437805176,\n",
       "  -0.3660949468612671,\n",
       "  0.40710094571113586,\n",
       "  -0.21621191501617432,\n",
       "  0.7460477948188782,\n",
       "  -0.007592581212520599,\n",
       "  0.7514299154281616,\n",
       "  0.20705705881118774,\n",
       "  -0.23565751314163208,\n",
       "  -0.8695673942565918,\n",
       "  0.695273220539093,\n",
       "  1.020877718925476,\n",
       "  -1.40951406955719,\n",
       "  -0.034863974899053574,\n",
       "  -0.8812468647956848,\n",
       "  -0.12503153085708618,\n",
       "  0.6216927766799927,\n",
       "  1.6123663187026978,\n",
       "  0.24773457646369934,\n",
       "  0.1790800392627716,\n",
       "  -0.43170875310897827,\n",
       "  -0.6205805540084839,\n",
       "  0.207245871424675,\n",
       "  -0.3290291726589203,\n",
       "  0.5001137256622314,\n",
       "  -0.47458741068840027,\n",
       "  0.6465418934822083,\n",
       "  -0.24411775171756744,\n",
       "  -0.22897562384605408,\n",
       "  -0.2533189654350281,\n",
       "  0.3114919066429138,\n",
       "  0.26585906744003296,\n",
       "  -0.15615858137607574,\n",
       "  -0.7636623382568359,\n",
       "  -0.23298923671245575,\n",
       "  0.4810052514076233,\n",
       "  -0.08756192028522491,\n",
       "  -1.5242388248443604,\n",
       "  0.37484461069107056,\n",
       "  -1.4944015741348267,\n",
       "  0.5932331681251526,\n",
       "  -0.28188416361808777,\n",
       "  -0.009261265397071838,\n",
       "  -0.22234848141670227,\n",
       "  0.08933795988559723,\n",
       "  -0.24419628083705902,\n",
       "  -1.074818730354309,\n",
       "  -0.05544675886631012,\n",
       "  -0.5016544461250305,\n",
       "  0.2462567389011383,\n",
       "  -0.6813366413116455,\n",
       "  0.12888391315937042,\n",
       "  0.1091809794306755,\n",
       "  0.5475212335586548,\n",
       "  0.3460717797279358,\n",
       "  0.03411615639925003,\n",
       "  -0.3813912868499756,\n",
       "  -0.03862150013446808,\n",
       "  -0.7905603051185608,\n",
       "  -0.3305177092552185,\n",
       "  -0.03546019643545151,\n",
       "  0.7849676012992859,\n",
       "  0.2148292511701584,\n",
       "  0.07293373346328735,\n",
       "  -0.6038781404495239,\n",
       "  -0.3207487165927887,\n",
       "  0.6504223942756653,\n",
       "  0.37865692377090454,\n",
       "  0.013278067111968994,\n",
       "  0.4914906322956085,\n",
       "  -0.7426963448524475,\n",
       "  -0.13479910790920258,\n",
       "  -0.28421300649642944,\n",
       "  -0.016220107674598694,\n",
       "  -0.3079613447189331,\n",
       "  0.20837074518203735,\n",
       "  0.44150054454803467,\n",
       "  0.6822079420089722,\n",
       "  -0.7398068904876709,\n",
       "  -0.021048884838819504,\n",
       "  0.1247468888759613,\n",
       "  0.3519490659236908,\n",
       "  0.6034411191940308,\n",
       "  0.5438865423202515,\n",
       "  0.29314354062080383,\n",
       "  -0.4435984194278717,\n",
       "  -0.2783024311065674,\n",
       "  -0.9134695529937744,\n",
       "  0.5368000268936157,\n",
       "  -0.4596679210662842,\n",
       "  -0.8980665802955627,\n",
       "  -0.1723909080028534,\n",
       "  0.34535151720046997,\n",
       "  0.003289930522441864,\n",
       "  -0.23769555985927582,\n",
       "  -0.14184007048606873,\n",
       "  0.24599075317382812,\n",
       "  0.3891782760620117,\n",
       "  0.3326709568500519,\n",
       "  0.46835169196128845,\n",
       "  -0.015082035213708878,\n",
       "  -0.4959118366241455,\n",
       "  -0.3449852168560028,\n",
       "  0.47260478138923645,\n",
       "  -0.7320908904075623,\n",
       "  -0.034329354763031006,\n",
       "  -0.09768460690975189,\n",
       "  -0.2757264971733093,\n",
       "  0.43966126441955566,\n",
       "  0.14818087220191956,\n",
       "  0.2766772210597992,\n",
       "  -0.1676088571548462,\n",
       "  0.3890470564365387,\n",
       "  -0.6460383534431458,\n",
       "  -0.3438621461391449,\n",
       "  -0.24112500250339508,\n",
       "  -1.0946362018585205,\n",
       "  -0.49078667163848877,\n",
       "  0.3162490427494049,\n",
       "  0.15528595447540283,\n",
       "  0.3084731698036194,\n",
       "  0.04235540330410004,\n",
       "  0.1893930733203888,\n",
       "  0.4499317407608032,\n",
       "  0.4387531578540802,\n",
       "  0.19620682299137115,\n",
       "  0.06953148543834686,\n",
       "  -0.2917560338973999,\n",
       "  0.393366277217865,\n",
       "  -0.21189576387405396,\n",
       "  1.1785829067230225,\n",
       "  -0.6754820346832275,\n",
       "  -0.2307634949684143,\n",
       "  -0.12322413921356201,\n",
       "  -0.3034279942512512,\n",
       "  -1.0664020776748657,\n",
       "  -0.3179863393306732,\n",
       "  -0.13206297159194946,\n",
       "  -0.07624629139900208,\n",
       "  -1.6311149597167969,\n",
       "  -0.7381948828697205,\n",
       "  0.300641804933548,\n",
       "  0.779454231262207,\n",
       "  -0.7029684782028198,\n",
       "  -0.19057315587997437,\n",
       "  -0.15462172031402588,\n",
       "  -0.2763722836971283,\n",
       "  -0.0593806616961956,\n",
       "  -0.46291422843933105,\n",
       "  0.5178301334381104,\n",
       "  -0.2241593301296234,\n",
       "  0.16755621135234833,\n",
       "  -0.5275987386703491,\n",
       "  0.32769373059272766,\n",
       "  0.6261113286018372,\n",
       "  -0.7819235920906067,\n",
       "  1.1780964136123657,\n",
       "  0.11397718638181686,\n",
       "  0.2868477702140808,\n",
       "  0.3936351239681244,\n",
       "  0.12458283454179764,\n",
       "  -0.25444263219833374,\n",
       "  -0.3436814546585083,\n",
       "  0.5253440737724304,\n",
       "  -0.16778621077537537,\n",
       "  -0.04729457199573517,\n",
       "  0.21766094863414764,\n",
       "  0.36254146695137024,\n",
       "  -1.111087679862976,\n",
       "  0.17225725948810577,\n",
       "  -0.37627893686294556,\n",
       "  0.5872035622596741,\n",
       "  -0.44974446296691895,\n",
       "  -0.03233586251735687,\n",
       "  -0.13319139182567596,\n",
       "  -0.4895780086517334,\n",
       "  0.1464313417673111,\n",
       "  0.3348119258880615,\n",
       "  -0.8743681907653809,\n",
       "  -0.49674224853515625,\n",
       "  -0.0860920250415802,\n",
       "  -0.400261253118515,\n",
       "  0.154832124710083,\n",
       "  -0.09847435355186462,\n",
       "  -0.3922538757324219,\n",
       "  -1.0346581935882568,\n",
       "  -0.8839322924613953,\n",
       "  0.11745451390743256,\n",
       "  0.027513537555933,\n",
       "  -0.13730008900165558,\n",
       "  0.39487016201019287,\n",
       "  0.856473982334137,\n",
       "  -0.12496702373027802,\n",
       "  0.46908026933670044,\n",
       "  -0.8911008834838867,\n",
       "  -0.06146756559610367,\n",
       "  0.5054237842559814,\n",
       "  -0.39930999279022217,\n",
       "  0.054325055330991745,\n",
       "  0.6156398057937622,\n",
       "  -0.18185895681381226,\n",
       "  0.666511058807373,\n",
       "  0.09511767327785492,\n",
       "  -0.03539128601551056,\n",
       "  0.23569297790527344,\n",
       "  -0.5642427802085876,\n",
       "  -0.15712706744670868,\n",
       "  -0.4020511209964752,\n",
       "  -0.5163153409957886,\n",
       "  -0.12480618059635162,\n",
       "  0.010540001094341278,\n",
       "  0.2423364222049713,\n",
       "  -0.002596568316221237,\n",
       "  0.27187126874923706,\n",
       "  -0.23473280668258667,\n",
       "  -0.6324582695960999,\n",
       "  -0.13354286551475525,\n",
       "  0.43514448404312134,\n",
       "  0.3383946418762207,\n",
       "  -0.9812421202659607,\n",
       "  -0.4800128936767578,\n",
       "  0.2413734644651413,\n",
       "  0.09094123542308807,\n",
       "  0.5330219268798828,\n",
       "  0.3906380236148834,\n",
       "  -0.6457760334014893,\n",
       "  -0.745042622089386,\n",
       "  0.18657279014587402,\n",
       "  -0.029745332896709442,\n",
       "  -0.18678569793701172,\n",
       "  -0.17368271946907043,\n",
       "  0.29828116297721863,\n",
       "  -0.4524613916873932,\n",
       "  0.009474024176597595,\n",
       "  0.004281703382730484,\n",
       "  -0.20395976305007935,\n",
       "  -0.06749038398265839,\n",
       "  0.56841641664505,\n",
       "  -0.424577534198761,\n",
       "  -1.0323491096496582,\n",
       "  0.354463666677475,\n",
       "  -0.33783912658691406,\n",
       "  1.0012480020523071,\n",
       "  -0.004710804671049118,\n",
       "  0.3097518980503082,\n",
       "  0.202389195561409,\n",
       "  -0.538200855255127,\n",
       "  -0.490435928106308,\n",
       "  0.798141360282898,\n",
       "  -0.6570843458175659,\n",
       "  0.18867771327495575,\n",
       "  0.6067916750907898,\n",
       "  0.5037318468093872,\n",
       "  0.041025470942258835,\n",
       "  0.5005432367324829,\n",
       "  -0.2362871766090393,\n",
       "  -0.17792171239852905,\n",
       "  0.029547369107604027,\n",
       "  0.03692857176065445,\n",
       "  -0.01201418787240982,\n",
       "  -0.6367518305778503,\n",
       "  0.6649658679962158,\n",
       "  -0.3548711836338043,\n",
       "  0.12059279531240463,\n",
       "  0.7874228358268738,\n",
       "  -0.12716607749462128,\n",
       "  -0.20078182220458984,\n",
       "  -0.5076302289962769,\n",
       "  -0.03614572435617447,\n",
       "  0.4240216016769409,\n",
       "  -0.5166892409324646,\n",
       "  0.30860185623168945,\n",
       "  -0.3156001567840576,\n",
       "  -0.7963800430297852,\n",
       "  -0.018968453630805016,\n",
       "  0.3674013018608093,\n",
       "  0.5218676328659058,\n",
       "  -0.033805690705776215,\n",
       "  -0.10888004302978516,\n",
       "  0.1239098533987999,\n",
       "  0.3489837050437927,\n",
       "  -0.5596467852592468,\n",
       "  0.39640113711357117,\n",
       "  -0.5952818393707275,\n",
       "  0.6137470006942749,\n",
       "  -1.2779009342193604,\n",
       "  1.0399658679962158,\n",
       "  0.5648753046989441,\n",
       "  0.5137826800346375,\n",
       "  1.3998901844024658,\n",
       "  -0.7700522541999817,\n",
       "  -0.7762561440467834,\n",
       "  0.48140406608581543,\n",
       "  1.481091022491455,\n",
       "  -0.15706877410411835,\n",
       "  -0.08445180952548981,\n",
       "  -1.1972007751464844,\n",
       "  0.2317052185535431,\n",
       "  -0.8780906796455383,\n",
       "  0.2693363428115845,\n",
       "  0.6326259970664978,\n",
       "  0.9804539084434509,\n",
       "  -0.6680471897125244,\n",
       "  -0.09419584274291992,\n",
       "  -0.3067472577095032,\n",
       "  0.29008135199546814,\n",
       "  1.0909925699234009,\n",
       "  -0.2810629606246948,\n",
       "  -0.524978518486023,\n",
       "  0.0772523283958435,\n",
       "  0.2510397434234619,\n",
       "  -0.5180385112762451,\n",
       "  -0.7335575222969055,\n",
       "  -0.5184672474861145,\n",
       "  -0.2385818064212799,\n",
       "  -0.3601700961589813,\n",
       "  0.34134432673454285,\n",
       "  -0.15210314095020294,\n",
       "  0.4983876049518585,\n",
       "  0.18802830576896667,\n",
       "  -0.20812448859214783,\n",
       "  -0.24281159043312073,\n",
       "  -0.42935389280319214,\n",
       "  -0.286432147026062,\n",
       "  0.11239992082118988,\n",
       "  -0.9663975834846497,\n",
       "  0.015732314437627792,\n",
       "  0.14785157144069672,\n",
       "  -0.07473856955766678,\n",
       "  -0.2252175509929657,\n",
       "  -0.6850072741508484,\n",
       "  -0.011227026581764221,\n",
       "  0.31120893359184265,\n",
       "  0.33367955684661865,\n",
       "  0.3412490785121918,\n",
       "  -0.29885780811309814,\n",
       "  0.1449587643146515,\n",
       "  -0.3646516799926758,\n",
       "  0.5979999899864197,\n",
       "  0.6060947179794312,\n",
       "  -0.17473077774047852,\n",
       "  -0.12495115399360657,\n",
       "  0.5900382399559021,\n",
       "  0.2532464265823364,\n",
       "  -0.8641400933265686,\n",
       "  1.1490652561187744,\n",
       "  -0.6199862360954285,\n",
       "  0.5037335157394409,\n",
       "  0.3915850520133972,\n",
       "  -0.5047248005867004,\n",
       "  0.061630070209503174,\n",
       "  -0.18094173073768616,\n",
       "  -0.2981307804584503,\n",
       "  0.9599073529243469,\n",
       "  0.5089473724365234,\n",
       "  0.7413637042045593,\n",
       "  1.3231871128082275,\n",
       "  0.1830274760723114,\n",
       "  -0.20374572277069092,\n",
       "  -0.1709403544664383,\n",
       "  -0.7192927598953247,\n",
       "  0.7104166150093079,\n",
       "  0.17623108625411987,\n",
       "  -0.2506633400917053,\n",
       "  -0.3220271170139313,\n",
       "  -0.37521934509277344,\n",
       "  -0.09160304069519043,\n",
       "  -0.48256397247314453,\n",
       "  -0.9037525653839111,\n",
       "  0.01616990566253662,\n",
       "  0.8013882637023926,\n",
       "  -0.73964923620224,\n",
       "  -0.23702472448349,\n",
       "  1.187371015548706,\n",
       "  -0.4849892854690552,\n",
       "  0.07441622018814087,\n",
       "  -0.9008283019065857,\n",
       "  -0.16636420786380768,\n",
       "  0.46956807374954224,\n",
       "  -0.25993645191192627,\n",
       "  -0.22495466470718384,\n",
       "  0.39793235063552856,\n",
       "  -0.28159084916114807,\n",
       "  1.0826025009155273,\n",
       "  0.39145568013191223,\n",
       "  0.19876495003700256,\n",
       "  -0.06505771726369858,\n",
       "  0.2744125425815582,\n",
       "  0.5141244530677795,\n",
       "  0.09626579284667969,\n",
       "  -0.3845325708389282,\n",
       "  -0.32376471161842346,\n",
       "  0.7189995050430298,\n",
       "  -0.4002304971218109,\n",
       "  1.0604544878005981,\n",
       "  0.09910570830106735,\n",
       "  0.4134445786476135,\n",
       "  0.015609817579388618,\n",
       "  -0.2128879725933075,\n",
       "  0.11960946023464203,\n",
       "  0.1542789340019226,\n",
       "  -0.3908771872520447,\n",
       "  -0.40836378931999207,\n",
       "  -0.48139360547065735,\n",
       "  -0.27607980370521545,\n",
       "  -0.2564578950405121,\n",
       "  0.4767870306968689,\n",
       "  -0.09707192331552505,\n",
       "  -0.6553539037704468,\n",
       "  -0.5735572576522827,\n",
       "  -0.3879642188549042,\n",
       "  0.46468377113342285,\n",
       "  0.2944408655166626,\n",
       "  0.22126121819019318,\n",
       "  0.1404333859682083,\n",
       "  -0.9050160646438599,\n",
       "  0.36836016178131104,\n",
       "  -0.026234589517116547,\n",
       "  0.7541248798370361,\n",
       "  0.5674129128456116,\n",
       "  -0.02836267650127411,\n",
       "  0.05686185508966446,\n",
       "  -0.46276819705963135,\n",
       "  0.2852136492729187,\n",
       "  -0.4020429849624634,\n",
       "  0.11113594472408295,\n",
       "  -0.0951957255601883,\n",
       "  0.075055792927742,\n",
       "  0.35061898827552795,\n",
       "  -0.4755782186985016,\n",
       "  0.08672407269477844,\n",
       "  0.8523955941200256,\n",
       "  0.07754219323396683,\n",
       "  -0.4853888750076294,\n",
       "  0.20417800545692444,\n",
       "  -0.25412482023239136,\n",
       "  -0.041174016892910004,\n",
       "  0.0863305926322937,\n",
       "  -1.1309328079223633,\n",
       "  -0.23657454550266266,\n",
       "  0.35126402974128723,\n",
       "  0.04264533519744873,\n",
       "  -0.17637205123901367,\n",
       "  -0.6336038112640381,\n",
       "  -0.021690264344215393,\n",
       "  -0.4188663363456726,\n",
       "  -1.2925044298171997,\n",
       "  0.4968548119068146,\n",
       "  0.6163820624351501,\n",
       "  -0.4240286946296692,\n",
       "  1.3108844757080078,\n",
       "  -0.4190079867839813,\n",
       "  0.5090375542640686,\n",
       "  0.09420636296272278,\n",
       "  -0.7687690854072571,\n",
       "  0.31249070167541504,\n",
       "  -0.0525037944316864,\n",
       "  -0.14756472408771515,\n",
       "  0.2454434335231781,\n",
       "  -0.12305282056331635,\n",
       "  0.016215160489082336,\n",
       "  0.021501731127500534,\n",
       "  0.4849412739276886,\n",
       "  -0.9922088384628296,\n",
       "  0.20269127190113068,\n",
       "  0.02073560655117035,\n",
       "  0.0004345625638961792,\n",
       "  -0.2778578996658325,\n",
       "  0.23136113584041595,\n",
       "  -0.894973635673523,\n",
       "  0.3265916109085083,\n",
       "  0.5162551403045654,\n",
       "  -0.47937995195388794,\n",
       "  -0.10203546285629272,\n",
       "  -0.439199835062027,\n",
       "  -0.3850316107273102,\n",
       "  0.6701965928077698,\n",
       "  0.6200487017631531,\n",
       "  3.7301859855651855,\n",
       "  0.5927343368530273,\n",
       "  0.8563047647476196,\n",
       "  0.24559074640274048,\n",
       "  0.07167086005210876,\n",
       "  -0.03234364464879036,\n",
       "  0.17380854487419128,\n",
       "  -0.023099955171346664,\n",
       "  -0.18491001427173615,\n",
       "  0.042299091815948486,\n",
       "  0.15457159280776978,\n",
       "  0.7063021659851074,\n",
       "  0.32277363538742065,\n",
       "  0.20496340095996857,\n",
       "  0.06927833706140518,\n",
       "  0.08988417685031891,\n",
       "  0.28534215688705444,\n",
       "  0.1872456818819046,\n",
       "  -0.30311691761016846,\n",
       "  0.3861648142337799,\n",
       "  0.5220154523849487,\n",
       "  -0.23053477704524994,\n",
       "  0.5905565023422241,\n",
       "  -0.22142091393470764,\n",
       "  -0.19763565063476562,\n",
       "  0.14543665945529938,\n",
       "  0.09767228364944458,\n",
       "  0.6410626173019409,\n",
       "  -0.6485235095024109,\n",
       "  -0.2918972373008728,\n",
       "  0.5641484260559082,\n",
       "  0.4196971654891968,\n",
       "  -0.084535151720047,\n",
       "  0.3810659945011139,\n",
       "  -0.8623392581939697,\n",
       "  -0.4032764136791229,\n",
       "  -0.08845916390419006,\n",
       "  0.8197926878929138,\n",
       "  -0.5557356476783752,\n",
       "  -0.48468834161758423,\n",
       "  -0.5825821757316589,\n",
       "  0.23640617728233337,\n",
       "  -0.5189698338508606,\n",
       "  -0.3959030210971832,\n",
       "  -0.2726607918739319,\n",
       "  0.16422748565673828,\n",
       "  0.9817534685134888,\n",
       "  -0.03940153867006302]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.post(\n",
    "    url=\"http://localhost:8080/triton_llm\",\n",
    "    params={\"model_name\": \"bert_model\"},\n",
    "    json={\"text\": \"example of the text\"},\n",
    ")\n",
    "\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c16c8",
   "metadata": {},
   "source": [
    "# dummy GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d560509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TorchScript модель успешно сохранена.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_ids = tokenizer(\"Hello, my name is\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "\n",
    "# Создаём класс-обёртку, чтобы избавиться от past_key_values\n",
    "class GPT2Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids=input_ids, use_cache=False)\n",
    "        return outputs.logits\n",
    "\n",
    "\n",
    "# Оборачиваем\n",
    "wrapped_model = GPT2Wrapper(model)\n",
    "\n",
    "# Трейсим\n",
    "traced = torch.jit.trace(wrapped_model, input_ids)\n",
    "traced.save(\"model.pt\")\n",
    "\n",
    "print(\"✅ TorchScript модель успешно сохранена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c43a853c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'once upon a time there was a great deal of talk about the possibility of a new world order.\\n\\nThe idea of a new'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.post(\n",
    "    url=\"http://localhost:8080/triton_gpt2\",\n",
    "    params={\n",
    "        \"model_name\": \"gpt2_model\",\n",
    "        \"max_new_tokens\": 20\n",
    "    },\n",
    "    json={\"text\": \"once upon a time there was a\"},\n",
    ")\n",
    "\n",
    "r.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
